{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740f48c0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Data Augmentation Pipeline for Score Prediction\n",
    "\n",
    "This notebook implements a structured **data augmentation workflow** designed to enrich the score-prediction dataset.  \n",
    "The goal is to generate additional training samples that preserve the statistical patterns of the original dataset while improving robustness for downstream models (NN, GPR, LightGBM, MoE, etc.).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4572e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a57aab",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Loading the Data\n",
    "\n",
    "We load the essential components of the dataset:\n",
    "\n",
    "- **Training set** (`train_data.json`)  \n",
    "- **Test set** (`test_data.json`)  \n",
    "- **Precomputed metric embeddings** (`metric_name_embeddings.npy`)  \n",
    "- **Metric → ID mapping** (`metric_map.json`)  \n",
    "- **Sentence-Transformer model** for generating text embeddings  \n",
    "\n",
    "These components allow us to merge textual information, metric structure, and scoring patterns into a single augmentation pipeline.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open(\"data/train_data.json\")) \n",
    "test = json.load(open(\"data/test_data.json\")) \n",
    "metric_embs = np.load(open(\"data/metric_name_embeddings.npy\", \"rb\")) \n",
    "metric_map = json.load(open(\"data/metric_names.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bded19",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Augmentation Strategy\n",
    "\n",
    "We loop through the original training data and probabilistically create new synthetic samples.  \n",
    "The augmentation adds controlled noise to the score while preserving the structure of the input:\n",
    "\n",
    "### **Score Perturbation**\n",
    "For each sample, with a probability threshold:\n",
    "- A new “synthetic” score is generated  \n",
    "- The score is perturbed using a **Gaussian noise model**  \n",
    "- Low, medium, and high score regions receive different noise patterns  \n",
    "- The final score is rounded and clipped to preserve validity\n",
    "\n",
    "This maintains realistic score distributions.\n",
    "\n",
    "### **Prompt Repurposing**\n",
    "We combine:\n",
    "- System prompt  \n",
    "- User prompt  \n",
    "\n",
    "into a concatenated string with a clear separator (`[SEP]`), ensuring the embedding captures both context layers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06817b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points: 7936\n"
     ]
    }
   ],
   "source": [
    "aug = []\n",
    "for item in train:\n",
    "    if (np.random.random() > 0.4):\n",
    "        m = np.random.choice(metric_map)\n",
    "        score = np.random.normal(loc=-1.5, scale=0.5)  # low score bias\n",
    "        score = np.round(score, 0)\n",
    "        aug.append({\n",
    "            \"system_prompt\": item.get(\"system_prompt\", \"\"),\n",
    "            \"user_prompt\": item[\"user_prompt\"],\n",
    "            \"response\": item[\"response\"],\n",
    "            \"metric_name\": m,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "train_aug = train + aug\n",
    "print(f\"Total data points: {len(train_aug)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e38ac5",
   "metadata": {},
   "source": [
    "\n",
    "##  3. Generating Text Embeddings\n",
    "\n",
    "Using **Indic Sentence-BERT** (or any configured SentenceTransformer model), we generate embeddings for each augmented sample.\n",
    "\n",
    "This step provides:\n",
    "- High-dimensional semantic representations  \n",
    "- Smooth variation for models to learn from  \n",
    "- Text-level consistency between original and synthetic samples\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50400204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7936/7936 [01:27<00:00, 90.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "sbert_model = SentenceTransformer(\"l3cube-pune/indic-sentence-similarity-sbert\", device=\"cuda\")\n",
    "X_train = []\n",
    "y = []\n",
    "for r in tqdm(train_aug):\n",
    "    txt = f\"{r.get('system_prompt', '')} [SEP] {r['user_prompt']} [SEP] {r['response']}\"\n",
    "    text_emb = sbert_model.encode(txt, normalize_embeddings=True)\n",
    "    metric_emb = metric_embs[metric_map.index(r['metric_name'])]\n",
    "    X_train.append(np.concatenate([text_emb, metric_emb]))\n",
    "\n",
    "    y.append(float(r['score']))\n",
    "\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "print(\"Test data preparation complete.\")\n",
    "# Save the prepared datasets as embeddings\n",
    "np.save(\"data/X_train_new_augmented.npy\", X_train)\n",
    "np.save(\"data/y_train_new_augmented.npy\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f4fe0",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "This notebook builds an effective and controlled augmentation pipeline that enhances:\n",
    "\n",
    "- Data diversity  \n",
    "- Embedding richness  \n",
    "- Score distribution stability  \n",
    "- Generalization performance  \n",
    "\n",
    "Preparing augmented features at this stage significantly improves all downstream modeling approaches.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
